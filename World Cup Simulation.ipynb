{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import time\n",
    "import http.cookiejar as cookiejar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import bs4 as bs\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "from scipy import stats\n",
    "\n",
    "style.use('seaborn-deep')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from datalab_beta import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this post, we will:\n",
    "\n",
    "- Create a crawler and pull team stats data from the web\n",
    "- Pre-process, explore and visualize that data\n",
    "- Download another type of data, with match results\n",
    "- Join those two datasets and build one model to guess match results\n",
    "- Create a simple Monte Carlo Simulation and get winner odds for the 2018 World Cup - Knockout Stage\n",
    "\n",
    "Some libraries we're going to use:\n",
    "\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Sklearn\n",
    "\n",
    "The World Cup is reaching a new stage and few were those who could anticipate what the spooky group stage unrolled.\n",
    "Now it's time for a much more thrilling phase, where the greatest of the world will face each other. The goal of this post is, using the power of data science, try to uncover some of the stats those games will present.\n",
    "\n",
    "The idea here is to make a machine learning algorithm to predict the winner of a single match, and from there, build a \n",
    "monte carlo simulation that could infer the odds of each knockout game winner, and subsequently, the probability for the world's champion.\n",
    "\n",
    "Be aware that this model is made for education purposes only, and should not be trusted for any kind of betting, even because the number of variables in a game are so huge you could not predict its result without a reasonable uncertainty amount.\n",
    "\n",
    "Most of the game simulations tend to use an overall number that represents a team's performance. Here we are trying a \n",
    "different aproach, using not only the overall, but 3 other values (Attack, Defense, Midside) altogether in a more sofisticated manner, to avoid simply converging all features to a single factor that dictates the team's strenght.\n",
    "\n",
    "This algorithm will be built on top of sklearn library, using Pandas as the library to manipulate data.\n",
    "\n",
    "So, the first step to obtain that data is to make a little crawler and get the information from fifaindex, a great source to pull international team stats back from 2005. I must admit the crawler ended up a bit lazy and it might take in some duplicates to our dataset. No concerns, though, since we can just drop those dupes with pandas later on (I'll also provide a link for downloading raw and processed datasets).\n",
    "\n",
    "I won't get into the very details of the way this scrapper was built, but the code will be left below if you'd like to check it out. In case you're curious about it, don't hesitate on reaching me out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_address = 'https://www.fifaindex.com/teams/fifa'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for day in range(1,260,1):\n",
    "    for pag in range(1,30):\n",
    "    \n",
    "        source_address = web_address + '05_' + str(day) + '/' + str(pag) + '/' + '?type=1'\n",
    "        print('Day:', str(day))\n",
    "        print(pag)\n",
    "\n",
    "        try:\n",
    "            soup = get_soup(source_address)\n",
    "            result_list =  soup.find('div', {'id': 'no-more-tables'})\n",
    "        except:\n",
    "            print('Page not found.')\n",
    "            break\n",
    "\n",
    "        date = str(soup.find('ol', {'class': 'breadcrumb'}))\n",
    "\n",
    "        if df.empty:\n",
    "            df = pd.read_html(str(result_list))[0]\n",
    "            df['date'] = date\n",
    "        else:\n",
    "            temp_df = pd.read_html(str(result_list))[0]\n",
    "            temp_df['date'] = date\n",
    "            df = df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Well, it seems that columns came out of order, and 'date' column looks really ugly, let's fix it to know what's \n",
    "going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['mid', 'ovr', 'att', 'def', 'league', '1', '2', '3', 'name', 'date']\n",
    "df = df[['date', 'name', 'att', 'def', 'mid', 'ovr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's fix those dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(s):\n",
    "    date = s.split('href=\"/teams/')[-1].split('>')[1].split('<')[0].split(',')[1]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df.date.apply(split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For simplicity sake, I'm going to average the performance of each team by year, so that we have few datapoints to handle. Feel free to keep the months and days and make a much more accurate model if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df.date.apply(text_norm)\n",
    "df['name'] = df.name.apply(text_norm)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop_duplicates()\n",
    "to_pickle(df, 'team_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_pickle('team_stats.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['name', 'date']).mean()\n",
    "df = df.reset_index()\n",
    "df.index = pd.DatetimeIndex(df.date).year\n",
    "df = df.drop(labels='date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "year_dic = {}\n",
    "\n",
    "for year in df.date.unique():\n",
    "    df_year = df[df.date == year]\n",
    "    df_year = df_year[df_year.ovr == df_year.ovr.max()]\n",
    "    year_dic[year] = [max(df_year.ovr.tolist()), df_year.name.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ovrs = []\n",
    "for year in df.date.unique():\n",
    "    df_year = df[df.date == year]\n",
    "    mean_ovrs.append(df_year.ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(year_dic.keys())\n",
    "max_ovrs = [i[0] for i in list(year_dic.values())]\n",
    "teams = [i[1] for i in list(year_dic.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_str = []\n",
    "for t in teams:\n",
    "    try:\n",
    "        teams_str.append(str(', '.join(t)))\n",
    "    except:\n",
    "        teams_str.append(t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's check the best performing teams for each date in a bar chart and see how they changed during the years. You can hover over each bar to check team's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='rodrigonader', api_key='MFs1Eh244N8I7ePAG3Va')\n",
    "\n",
    "trace1= go.Bar(\n",
    "            x=years,\n",
    "            y=max_ovrs,\n",
    "            text=teams_str,\n",
    "            name = 'Max Performance',\n",
    "            marker=dict(\n",
    "            color='rgb(103,113,242)')\n",
    "    )\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = years,\n",
    "    y = mean_ovrs,\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Mean Performance',\n",
    "    marker = dict(\n",
    "        size = 12,\n",
    "        color = 'rgba(300, 300, 300, .9)',\n",
    "        line = dict(\n",
    "            width = 2)\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Team performance by year',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='text-hover-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for year in years:\n",
    "    \n",
    "    values = df.ovr[df.date==year].tolist()\n",
    "    \n",
    "\n",
    "    trace1 = go.Scatter(\n",
    "                x=[year] * len(values),\n",
    "                y=values,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                color='rgb(150,150,150)')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    data.append(trace1)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "py.iplot(fig, filename='text-hover-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais vizualizações aqui"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's now open the csv file containing match result information of international teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('match_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.drop(['city', 'tournament', 'country'], axis=1)\n",
    "results.home_team = results.home_team.apply(text_norm)\n",
    "results.away_team = results.away_team.apply(text_norm)\n",
    "results.index = pd.DatetimeIndex(results.date).year\n",
    "results = results.drop('date', 1)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 steps are going to be important to start cleaning this data. First, as we only have performance data for 2004 on, \n",
    "we should get rid of all those other years. Second, we shouldn't be working with teams that aren't in the df \n",
    "dataframe (our main dataframe), since we don't have stats for them. Finally we have to work around this home-away \n",
    "team situation, since for world cup prediction we are going to consider all teams being away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.loc[2004:2017]\n",
    "results.tail()\n",
    "\n",
    "df_teams = list(df.name.unique())\n",
    "results = results.reset_index()\n",
    "\n",
    "for index, row in results.iterrows():\n",
    "    if row.home_team not in df_teams:\n",
    "        results.loc[index, 'home_team'] = None\n",
    "    if row.away_team not in df_teams:\n",
    "        results.loc[index, 'away_team'] = None\n",
    "        \n",
    "results = results.dropna()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the last problem we are going to consider all world cup games as neutral.\n",
    "Let's now input the teams stats into the results dataframe, we are going to do this by creating 8 new columns in \n",
    "the results dataframe, representing 4 skills for each team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['att1'] = np.nan\n",
    "results['def1'] = np.nan\n",
    "results['mid1'] = np.nan\n",
    "results['ovr1'] = np.nan\n",
    "results['att2'] = np.nan\n",
    "results['def2'] = np.nan\n",
    "results['mid2'] = np.nan\n",
    "results['ovr2'] = np.nan\n",
    "\n",
    "for index, row in results.iterrows():\n",
    "    date = row.date\n",
    "    team1 = row.home_team\n",
    "    team2 = row.away_team\n",
    "    try:\n",
    "        results.loc[index, 'att1'] = df[df.date == date][df.name == team1]['att'].iloc[0]\n",
    "        results.loc[index, 'def1'] = df[df.date == date][df.name == team1]['def'].iloc[0]\n",
    "        results.loc[index, 'mid1'] = df[df.date == date][df.name == team1]['mid'].iloc[0]\n",
    "        results.loc[index, 'ovr1'] = df[df.date == date][df.name == team1]['ovr'].iloc[0]\n",
    "        results.loc[index, 'att2'] = df[df.date == date][df.name == team2]['att'].iloc[0]\n",
    "        results.loc[index, 'def2'] = df[df.date == date][df.name == team2]['def'].iloc[0]\n",
    "        results.loc[index, 'mid2'] = df[df.date == date][df.name == team2]['mid'].iloc[0]\n",
    "        results.loc[index, 'ovr2'] = df[df.date == date][df.name == team2]['ovr'].iloc[0]\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally we need to compact the number of goals of each team in a single feature, which is the target to be predicted.\n",
    "Let's just subtract home_score from away_score, so that we have one score representing which team wins (if score > 0,\n",
    "that means home_team won, otherwise away_team won). Notice that I'm using the terms home and away simply because this\n",
    "is the way this dataset has come, but this will not influence in our analysis, as we'll work only with neutral games in the regression. Anyway the terms home and away are going to stick for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['score'] = results.home_score - results.away_score\n",
    "# results = results.drop(['home_score', 'away_score', 'home_team', 'away_team'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['winner'] = None\n",
    "results['winner'][results.score > 0] = 1\n",
    "results['winner'][results.score < 0] = -1\n",
    "results['winner'][results.score == 0] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Just to simplify the problem a little more, I'm going to help the model extracting the differences between abilities, \n",
    "instead of providing the performance itself, we're going to provide the performance contrast between matching teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['att'] = results['att1'] - results['att2']\n",
    "results['def'] = results['def1'] - results['def2']\n",
    "results['mid'] = results['mid1'] - results['mid2']\n",
    "results['ovr'] = results['ovr1'] - results['ovr2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results.winner != 0]\n",
    "\n",
    "to_drop = results[results.winner == 1].sample(247)\n",
    "results = results.drop(labels=to_drop.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = results.ovr1,\n",
    "    y = results.score,\n",
    "    mode = 'markers',\n",
    "    name = 'Mean Performance',\n",
    "    marker = dict(\n",
    "        size = 12,\n",
    "        color = 'rgba(300, 300, 300, .9)',\n",
    "        line = dict(\n",
    "            width = 2)\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "py.iplot(fig, filename='text-hover-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter3d(\n",
    "    x = results.att,\n",
    "    y = results.def1,\n",
    "    z = results.mid1,\n",
    "    mode = 'markers',\n",
    "    name = 'Mean Performance',\n",
    "    marker = dict(\n",
    "        size = np.power(results.score.tolist(), 2),\n",
    "        color = 'rgba(100, 150, 300, .9)',\n",
    "        line = dict(\n",
    "            width = 1)\n",
    "        )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = {\n",
    "  \"margin\": {\n",
    "    \"r\": 10, \n",
    "    \"t\": 25, \n",
    "    \"b\": 40, \n",
    "    \"l\": 60\n",
    "  }, \n",
    "  \"paper_bgcolor\": \"rgb(243, 243, 243)\", \n",
    "  \"plot_bgcolor\": \"rgb(243, 243, 243)\", \n",
    "  \"scene\": {\n",
    "    \"xaxis\": {\n",
    "      \"gridcolor\": \"rgb(255, 255, 255)\", \n",
    "      \"gridwidth\": 2, \n",
    "      \"ticklen\": 5, \n",
    "      \"title\": \"Attack\", \n",
    "      \"type\": \"log\", \n",
    "      \"zerolinewidth\": 1\n",
    "    }, \n",
    "    \"yaxis\": {\n",
    "      \"gridcolor\": \"rgb(255, 255, 255)\", \n",
    "      \"ticklen\": 5, \n",
    "      \"title\": \"Defense\", \n",
    "      \"zerolinewidth\": 1\n",
    "    }, \n",
    "    \"zaxis\": {\n",
    "      \"gridcolor\": \"rgb(255, 255, 255)\", \n",
    "      \"ticklen\": 5, \n",
    "      \"title\": \"Midside\", \n",
    "      \"type\": \"log\", \n",
    "      \"zerolinewidth\": 1\n",
    "    }\n",
    "  }, \n",
    "  \"title\": \"Teams Performance 3D\", \n",
    "  \"xaxis\": {\"domain\": [0, 1]}, \n",
    "  \"yaxis\": {\"domain\": [0, 1]}\n",
    "}\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# fig = go.Figure(data=data)\n",
    "\n",
    "py.iplot(fig, filename='text-hover-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pickle(results, 'results_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(data, target_col='price_area'):\n",
    "\n",
    "\tfeatures = data.drop(target_col, 1)\n",
    "\ttarget = data[target_col]\n",
    "\tx = to_array(features)\n",
    "\n",
    "\ty = to_array(target)\n",
    "\n",
    "\treturn x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.winner = results.winner.apply(to_float)\n",
    "\n",
    "results.replace(np.inf, np.nan)\n",
    "results = results.dropna()\n",
    "\n",
    "results = results.sample(frac=1)\n",
    "\n",
    "data = results.drop(['date', 'score'], 1)\n",
    "data = results[['def', 'ovr', 'att', 'mid', 'winner']]\n",
    "train, test = train_test_split(data, 0.2)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "x_train, y_train = prepare(train, target_col='winner')\n",
    "x_test, y_test = prepare(test, target_col='winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This reveals that, althought it may seem that the overall is the consolidation of the other 3 attributes, this is not,\n",
    "actually true, since, when we remove those other features, the regressor performs less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, rf.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, svc.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prediction'] = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's check our model performance for each class independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_score = accuracy_score(test.winner[test.winner > 0].values, test.prediction[test.winner > 0].values)\n",
    "second_score = accuracy_score(test.winner[test.winner < 0].values, test.prediction[test.winner < 0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1= go.Bar(\n",
    "            x=['First Team Wins', 'Second Team Wins'],\n",
    "            y=[first_score, second_score],\n",
    "            name = 'Model Scores',\n",
    "            marker=dict(\n",
    "            color='rgb(50,300,160)'\n",
    "            )\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(data, filename='Model-Scores')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have a little more accuracy on the first team, that may be related to the fact that in the original dataset, this team is the home team, and it could affect the behavior of the model, but apart from that, it could be simply random as well, so let's move on.\n",
    "\n",
    "That's ok. We have a predictor that can guess with 70% accuracy which team is going to win. Enough for building a \n",
    "simulation. Shall we?\n",
    "\n",
    "First thing we need is performance data for world cup teams that were classified in groups stage. We'll build a scraper similar to the one we built for the other years, but now using 2018 World Cup performance data\n",
    "from fifaindex. \n",
    "\n",
    "Unfortunately it seems that this data does not differ much from 2018 (non world cup) data. Since Germany still ocupies\n",
    "the second place in the ranking, while in reality it's already out of the competition. Anyway we'll stick to this source to gather our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_address = 'https://www.fifaindex.com/teams/fifa18wc_262/'\n",
    "\n",
    "wc = pd.DataFrame()\n",
    "\n",
    "for pag in range(1,30):\n",
    "\n",
    "    source_address = web_address + str(pag) + '/' + '?type=1'\n",
    "    print(pag)\n",
    "\n",
    "    try:\n",
    "        soup = get_soup(source_address)\n",
    "        result_list =  soup.find('div', {'id': 'no-more-tables'})\n",
    "    except:\n",
    "        print('Page not found.')\n",
    "        break\n",
    "\n",
    "    if df.empty:\n",
    "        wc = pd.read_html(str(result_list))[0]\n",
    "        wc['date'] = date\n",
    "    else:\n",
    "        temp_wc = pd.read_html(str(result_list))[0]\n",
    "        temp_wc['date'] = date\n",
    "        wc = wc.append(temp_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.columns = ['0', 'name', '1', 'att', 'mid', 'def', 'ovr', '2', '3', '4']\n",
    "wc = wc[['name', 'att', 'def', 'mid', 'ovr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.name = wc.name.apply(text_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = read_pickle('world_cup_teams.pickle')\n",
    "wc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(wc, team1, team2, model):\n",
    "    \n",
    "    match = pd.DataFrame(columns=['att1','def1','mid1','ovr1','att2','def2','mid2','ovr2'], index=[0])\n",
    "    \n",
    "    match['att1'] = wc[wc.name == team1]['att'].iloc[0]\n",
    "    match['def1'] = wc[wc.name == team1]['def'].iloc[0]\n",
    "    match['mid1'] = wc[wc.name == team1]['mid'].iloc[0]\n",
    "    match['ovr1'] = wc[wc.name == team1]['ovr'].iloc[0]\n",
    "\n",
    "    match['att2'] = wc[wc.name == team2]['att'].iloc[0]\n",
    "    match['def2'] = wc[wc.name == team2]['def'].iloc[0]\n",
    "    match['mid2'] = wc[wc.name == team2]['mid'].iloc[0]\n",
    "    match['ovr2'] = wc[wc.name == team2]['ovr'].iloc[0]\n",
    "    \n",
    "    match['att'] = match['att1'] - match['att2']\n",
    "    match['def'] = match['def1'] - match['def2']\n",
    "    match['mid'] = match['mid1'] - match['mid2']\n",
    "    match['ovr'] = match['ovr1'] - match['ovr2']\n",
    "    \n",
    "    match = match[['att', 'def', 'mid', 'ovr']]\n",
    "    \n",
    "    match_array = match.values\n",
    "    \n",
    "    prediction = model.predict(match_array)\n",
    "    \n",
    "    winner = None\n",
    "    \n",
    "    if prediction == 1:\n",
    "        winner = team1\n",
    "    elif prediction == -1:\n",
    "        winner = team2\n",
    "    \n",
    "    return winner\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match(wc, 'brazil', 'spain', svc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Oh I see. Didn't expect that sad outcome for us! But there's a major problem here, team performance vary a lot, and the firt phase of this cup is proof. So let's add some randomness to it, so that results will not be always the same every time we simulate a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(wc, team1, team2, model, random_scale=5):\n",
    "    \n",
    "    match = pd.DataFrame(columns=['att1','def1','mid1','ovr1','att2','def2','mid2','ovr2'], index=[0])\n",
    "    \n",
    "    att1 = wc[wc.name == team1]['att'].iloc[0]\n",
    "    def1 = wc[wc.name == team1]['def'].iloc[0]\n",
    "    mid1 = wc[wc.name == team1]['mid'].iloc[0]\n",
    "    ovr1 = wc[wc.name == team1]['ovr'].iloc[0]\n",
    "\n",
    "    att2 = wc[wc.name == team2]['att'].iloc[0]\n",
    "    def2 = wc[wc.name == team2]['def'].iloc[0]\n",
    "    mid2 = wc[wc.name == team2]['mid'].iloc[0]\n",
    "    ovr2 = wc[wc.name == team2]['ovr'].iloc[0]\n",
    "    \n",
    "    match['att1'] = np.random.normal(att1, scale=random_scale)\n",
    "    match['def1'] = np.random.normal(def1, scale=random_scale)\n",
    "    match['mid1'] = np.random.normal(mid1, scale=random_scale)\n",
    "    match['ovr1'] = np.random.normal(ovr1, scale=random_scale)\n",
    "\n",
    "    match['att2'] = np.random.normal(att2, scale=random_scale)\n",
    "    match['def2'] = np.random.normal(def2, scale=random_scale)\n",
    "    match['mid2'] = np.random.normal(mid2, scale=random_scale)\n",
    "    match['ovr2'] = np.random.normal(ovr2, scale=random_scale)\n",
    "    \n",
    "    match['att'] = match['att1'] - match['att2']\n",
    "    match['def'] = match['def1'] - match['def2']\n",
    "    match['mid'] = match['mid1'] - match['mid2']\n",
    "    match['ovr'] = match['ovr1'] - match['ovr2']\n",
    "    \n",
    "    match = match[['att', 'def', 'mid', 'ovr']]\n",
    "    \n",
    "    match_array = match.values\n",
    "    \n",
    "    prediction = model.predict(match_array)\n",
    "    \n",
    "    winner = None\n",
    "    \n",
    "    if prediction == 1:\n",
    "        winner = team1\n",
    "    elif prediction == -1:\n",
    "        winner = team2\n",
    "    \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, random_scale will be the factor that determines how much randomness we want to apply to a team's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_matches(team1, team2, n_matches=10000):\n",
    "    \n",
    "    match_results = []\n",
    "    for i in range(n_matches):\n",
    "        match_results.append(match(wc, team1, team2, svc, random_scale=5))\n",
    "        \n",
    "    team1_proba = match_results.count(team1) / len(match_results) * 100\n",
    "    team2_proba = match_results.count(team2) / len(match_results) * 100\n",
    "    \n",
    "    print(team1, str(round(team1_proba, 2)) + '%')\n",
    "    print(team2, str(round(team2_proba,2)) + '%')\n",
    "    print('-------------------------')\n",
    "    print()\n",
    "    \n",
    "    if team1_proba > team2_proba:\n",
    "        overall_winner = team1\n",
    "    else:\n",
    "        overall_winner = team2\n",
    "    \n",
    "    return {'team1': team1,\n",
    "            'team2': team2,\n",
    "            'team1_proba': team1_proba, \n",
    "            'team2_proba': team2_proba, \n",
    "            'overall_winner': overall_winner,\n",
    "            'match_results': match_results}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_test = simulate_matches('croatia', 'denmark', n_matches=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = []\n",
    "for i in range(len(simulation_test['match_results'])):\n",
    "    denmark = simulation_test['match_results'][:i].count('denmark') / (i+1) * 100\n",
    "    croatia = simulation_test['match_results'][:i].count('croatia') / (i+1) * 100\n",
    "    p_list.append(denmark - croatia)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = p_list,\n",
    "    name='Teams divergence by number of simulations',\n",
    "    marker= dict(color='rgb(230,90,110)'))\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "py.iplot(data, filename='Divergence')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see that winner probabilities stabilize around 8k match simulations, so that's the value we're going to use. \n",
    "Let's build the championship tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Round of 16:')\n",
    "\n",
    "ko1 = simulate_matches('uruguay', 'portugal', n_matches=8000)['overall_winner']\n",
    "ko2 = simulate_matches('france', 'argentina', n_matches=8000)['overall_winner']\n",
    "ko3 = simulate_matches('brazil', 'mexico', n_matches=8000)['overall_winner']\n",
    "ko4 = simulate_matches('belgium', 'japan', n_matches=8000)['overall_winner']\n",
    "ko5 = simulate_matches('spain', 'russia', n_matches=8000)['overall_winner']\n",
    "ko6 = simulate_matches('croatia', 'denmark', n_matches=8000)['overall_winner']\n",
    "ko7 = simulate_matches('sweden', 'switzerland', n_matches=8000)['overall_winner']\n",
    "ko8 = simulate_matches('colombia', 'england', n_matches=8000)['overall_winner']\n",
    "\n",
    "print()\n",
    "print('Quarter Finals:')\n",
    "print()\n",
    "\n",
    "quarters1 = simulate_matches(ko1, ko2, n_matches=8000)['overall_winner']\n",
    "quarters2 = simulate_matches(ko3, ko4, n_matches=8000)['overall_winner']\n",
    "quarters3 = simulate_matches(ko5, ko6, n_matches=8000)['overall_winner']\n",
    "quarters4 = simulate_matches(ko7, ko8, n_matches=8000)['overall_winner']\n",
    "\n",
    "print()\n",
    "print('Semi Finals:')\n",
    "print()\n",
    "\n",
    "semifinals1 = simulate_matches(quarters1, quarters2, n_matches=8000)['overall_winner']\n",
    "semifinals2 = simulate_matches(quarters3, quarters4, n_matches=8000)['overall_winner']\n",
    "\n",
    "print()\n",
    "print('Finals:')\n",
    "print()\n",
    "\n",
    "finals = simulate_matches(semifinals1, semifinals2, n_matches=8000)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Considering that our model has an error of 30%, let's calculate the chance of Spain beating all teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_proba = 0.817 * 0.7 * 0.862 * 0.7 * 0.718 * 0.7 * 0.593 * 0.7 * 100\n",
    "\n",
    "print('Chance of Spain winning:', str(round(spain_proba,2)) + '%')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Well, it seems quite fair.\n",
    "\n",
    "\n",
    "That's it for this article. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "\n",
    "- Try more complex ML algorithms\n",
    "- Gather more data: national team stats and matches\n",
    "- Go even further and make new model for team evaluation based on player stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
